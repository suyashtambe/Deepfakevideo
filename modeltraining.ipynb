{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_limit=30, resize=(224, 224)):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    count = 0\n",
    "    while video.isOpened() and count < frame_limit:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, resize)\n",
    "        frame = img_to_array(frame)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "        count += 1\n",
    "    video.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    cnn_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(cnn_base)\n",
    "    cnn_model.add(Flatten())\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(cnn_model, input_shape=input_shape))\n",
    "    model.add(LSTM(256, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r'C:\\Users\\Suyash Tambe\\Desktop\\Deepfakevideo\\train dataset\\eqjscdagiv.mp4'\n",
    "frames = extract_frames(video_path, frame_limit=100)\n",
    "X_input = np.expand_dims(frames, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Suyash Tambe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33s/step\n",
      "0.54749846\n"
     ]
    }
   ],
   "source": [
    "input_shape = (30, 224, 224, 3)\n",
    "\n",
    "model = build_model(input_shape)\n",
    "\n",
    "prediction = model.predict(X_input)\n",
    "\n",
    "print(prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is RIYAL.\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "\n",
    "if prediction[0][0] > threshold:\n",
    "    print(\"The video is a DEEP FAKE.\")                               \n",
    "if prediction[0][0] < threshold:\n",
    "    print(\"The video is RIYAL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cogvlm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Suyash Tambe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Suyash Tambe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Flatten, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential  # Import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from transformers import TFBertModel, BertTokenizer  # For language part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and BERT model for language processing (COG-VLM component)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract frames from the video and preprocess them\n",
    "def extract_frames(video_path, frame_limit=30, resize=(224, 224)):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    count = 0\n",
    "    while video.isOpened() and count < frame_limit:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, resize)\n",
    "        frame = img_to_array(frame)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "        count += 1\n",
    "    video.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the language input (e.g., description or prompt)\n",
    "def preprocess_language_input(text, max_length=30):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", max_length=max_length, truncation=True, padding='max_length')\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vision model using VGG16 and LSTM (for temporal sequence)\n",
    "def build_cogvlm_model(input_shape, text_input_shape, bert_model):\n",
    "    # CNN Model for visual feature extraction\n",
    "    cnn_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    cnn_model = Sequential()  # Use Sequential for the CNN model\n",
    "    cnn_model.add(cnn_base)\n",
    "    cnn_model.add(Flatten())\n",
    "\n",
    "    # Video input (image sequences)\n",
    "    video_input = Input(shape=input_shape, name=\"video_input\")\n",
    "    video_features = TimeDistributed(cnn_model)(video_input)\n",
    "    video_features = LSTM(256, return_sequences=False)(video_features)\n",
    "    video_features = Dropout(0.5)(video_features)\n",
    "\n",
    "    # Language input (text, using BERT)\n",
    "    text_input_ids = Input(shape=text_input_shape, name=\"text_input_ids\", dtype='int32')\n",
    "    text_attention_mask = Input(shape=text_input_shape, name=\"text_attention_mask\", dtype='int32')\n",
    "    \n",
    "    # Convert Keras tensors to TensorFlow tensors\n",
    "    text_input_ids_tensor = tf.convert_to_tensor(text_input_ids)\n",
    "    text_attention_mask_tensor = tf.convert_to_tensor(text_attention_mask)\n",
    "\n",
    "    # BERT model for processing text input\n",
    "    text_features = bert_model(input_ids=text_input_ids_tensor, attention_mask=text_attention_mask_tensor)[1]  # Get the pooled output (CLS token)\n",
    "\n",
    "    # Combine video and text features\n",
    "    combined_features = Concatenate()([video_features, text_features])\n",
    "\n",
    "    # Final output layer (binary classification)\n",
    "    output = Dense(1, activation='sigmoid')(combined_features)\n",
    "\n",
    "    # Define the full model\n",
    "    model = Model(inputs=[video_input, text_input_ids, text_attention_mask], outputs=output)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with a video and a text prompt\n",
    "video_path = r'C:\\Users\\Suyash Tambe\\Desktop\\Deepfakevideo\\train dataset\\eqjscdagiv.mp4'\n",
    "frames = extract_frames(video_path, frame_limit=100)\n",
    "\n",
    "# Language description (replace with the actual text context for your video)\n",
    "text_description = \"This is a deepfake video example.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# For 100 frames, 224x224 image size, 3 channels\u001b[39;00m\n\u001b[0;32m     11\u001b[0m text_input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m30\u001b[39m,)  \u001b[38;5;66;03m# Based on BERT input max length\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_cogvlm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_input_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Make predictions on video + text\u001b[39;00m\n\u001b[0;32m     16\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([X_input_video, input_ids, attention_mask])\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mbuild_cogvlm_model\u001b[1;34m(input_shape, text_input_shape, bert_model)\u001b[0m\n\u001b[0;32m     17\u001b[0m text_attention_mask \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39mtext_input_shape, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert Keras tensors to TensorFlow tensors\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m text_input_ids_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m text_attention_mask_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(text_attention_mask)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# BERT model for processing text input\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Suyash Tambe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Suyash Tambe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "# Preprocess language input\n",
    "input_ids, attention_mask = preprocess_language_input(text_description, max_length=30)\n",
    "\n",
    "# Expand dimensions for batch size\n",
    "X_input_video = np.expand_dims(frames, axis=0)\n",
    "input_ids = np.expand_dims(input_ids, axis=0)\n",
    "attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "\n",
    "# Build the COG-VLM model\n",
    "input_shape = (100, 224, 224, 3)  # For 100 frames, 224x224 image size, 3 channels\n",
    "text_input_shape = (30,)  # Based on BERT input max length\n",
    "\n",
    "model = build_cogvlm_model(input_shape, text_input_shape, bert_model)\n",
    "\n",
    "# Make predictions on video + text\n",
    "prediction = model.predict([X_input_video, input_ids, attention_mask])\n",
    "\n",
    "# Print the prediction result\n",
    "print(f\"Prediction: {prediction[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
